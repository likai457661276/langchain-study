#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹é€‰æ‹©åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from rich.console import Console
from rich.panel import Panel
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©å‡½æ•°
sys.path.append(str(Path(__file__).resolve().parent))
from simple_chatbot import list_available_models, initialize_siliconflow_model

console = Console()

def test_model_selection():
    """
    æµ‹è¯•æ¨¡å‹é€‰æ‹©åŠŸèƒ½
    """
    console.print(Panel.fit("ğŸ§ª æ¨¡å‹é€‰æ‹©æµ‹è¯•", style="bold blue"))
    
    models = list_available_models()
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for i, model in enumerate(models, 1):
        console.print(f"\n[bold]æµ‹è¯•æ¨¡å‹ {i}/{len(models)}: {model['name']}[/bold]")
        
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            llm = initialize_siliconflow_model(model['id'])
            
            # å‘é€ç®€å•æµ‹è¯•æ¶ˆæ¯
            test_message = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"
            console.print(f"å‘é€æ¶ˆæ¯: {test_message}")
            
            response = llm.invoke([HumanMessage(content=test_message)])
            console.print(f"[green]âœ“ å“åº”æˆåŠŸ:[/green] {response.content[:100]}...")
            
        except Exception as e:
            console.print(f"[red]âœ— é”™è¯¯:[/red] {str(e)}")
    
    console.print("\n[bold green]æµ‹è¯•å®Œæˆï¼[/bold green]")

if __name__ == "__main__":
    test_model_selection()
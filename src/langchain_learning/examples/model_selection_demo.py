#!/usr/bin/env python3
"""
æ¨¡å‹é€‰æ‹©æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„æ¨¡å‹è¿›è¡Œå¯¹è¯
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©å‡½æ•°
sys.path.append(str(Path(__file__).resolve().parent))
from simple_chatbot import list_available_models, initialize_siliconflow_model

console = Console()

def compare_models():
    """
    æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å“åº”
    """
    console.print(Panel.fit("ğŸ” æ¨¡å‹æ¯”è¾ƒæ¼”ç¤º", style="bold blue"))
    
    # æµ‹è¯•é—®é¢˜
    test_question = "è¯·ç”¨ä¸­æ–‡è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚"
    
    models = list_available_models()
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
    table = Table(title="æ¨¡å‹å“åº”æ¯”è¾ƒ")
    table.add_column("æ¨¡å‹", style="cyan", no_wrap=True)
    table.add_column("å“åº”", style="green")
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model in models:
        console.print(f"\n[bold]æ­£åœ¨æµ‹è¯•æ¨¡å‹:[/bold] {model['name']}")
        
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            llm = initialize_siliconflow_model(model['id'])
            
            # å‘é€æµ‹è¯•æ¶ˆæ¯
            console.print(f"å‘é€é—®é¢˜: {test_question}")
            
            response = llm.invoke([HumanMessage(content=test_question)])
            
            # æ·»åŠ åˆ°è¡¨æ ¼
            table.add_row(model['name'], response.content[:200] + "..." if len(response.content) > 200 else response.content)
            
        except Exception as e:
            console.print(f"[red]âœ— é”™è¯¯:[/red] {str(e)}")
            table.add_row(model['name'], f"[red]é”™è¯¯: {str(e)}[/red]")
    
    # æ˜¾ç¤ºæ¯”è¾ƒè¡¨æ ¼
    console.print("\n")
    console.print(table)
    
    console.print("\n[bold green]æ¼”ç¤ºå®Œæˆï¼[/bold green]")

def interactive_model_selection():
    """
    äº¤äº’å¼æ¨¡å‹é€‰æ‹©æ¼”ç¤º
    """
    console.print(Panel.fit("ğŸ¤– äº¤äº’å¼æ¨¡å‹é€‰æ‹©æ¼”ç¤º", style="bold blue"))
    
    models = list_available_models()
    
    console.print("\n[bold]å¯ç”¨æ¨¡å‹:[/bold]")
    for i, model in enumerate(models, 1):
        console.print(f"{i}. {model['name']}")
    
    choice = console.input("\n[bold]é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯ (1-4):[/bold] ")
    
    try:
        choice_idx = int(choice) - 1
        if 0 <= choice_idx < len(models):
            selected_model = models[choice_idx]
            console.print(f"\n[green]å·²é€‰æ‹©:[/green] {selected_model['name']}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            llm = initialize_siliconflow_model(selected_model['id'])
            
            # è¿›è¡Œç®€å•å¯¹è¯
            console.print("\n[bold]å¼€å§‹å¯¹è¯ (è¾“å…¥ 'é€€å‡º' ç»“æŸ):[/bold]")
            
            while True:
                user_input = console.input("\n[bold]ä½ :[/bold] ")
                
                if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
                    console.print("[green]å†è§ï¼[/green]")
                    break
                
                response = llm.invoke([HumanMessage(content=user_input)])
                console.print(f"[bold]{selected_model['name']}:[/bold] {response.content}")
        else:
            console.print("[red]æ— æ•ˆé€‰æ‹©ã€‚[/red]")
    except ValueError:
        console.print("[red]è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ã€‚[/red]")

if __name__ == "__main__":
    console.print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    console.print("1. æ¨¡å‹å“åº”æ¯”è¾ƒ")
    console.print("2. äº¤äº’å¼æ¨¡å‹é€‰æ‹©")
    
    mode = console.input("\n[bold]è¾“å…¥é€‰æ‹© (1-2):[/bold] ")
    
    if mode == "1":
        compare_models()
    elif mode == "2":
        interactive_model_selection()
    else:
        console.print("[red]æ— æ•ˆé€‰æ‹©ã€‚[/red]")